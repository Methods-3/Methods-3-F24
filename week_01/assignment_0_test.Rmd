---
title: "assignment_0_test"
date: "2024-09-06"
output: html_document
---

# Exercises and objectives

The objectives of today's exercises are:\
1) Check that your environment works by running single level ...
2) ... and multilevel models in R
3) Run a single level model in Python

We are going to fit some single level and multilevel models on the *sleepstudy* data from the *lme4* package.\
Check out the data description and format using `?sleepstudy`

```{r, eval=FALSE}
library(lme4)
data("sleepstudy")
```

## Exercise 1

Do a linear regression, modelling reaction time as dependent of number of days of sleep deprivation using the function *lm*\
(The General Linear Model: $Y = X\beta + \epsilon; \epsilon$ \~ $N(0, \sigma^2)$)

```{r, eval=FALSE}
single.level.model <- lm(formula=..., data=...)
```

1.  Extract $\hat{\beta}$, $Y$, $\hat{Y}$, $x$, $X$, $\hat{\sigma}$ and $\epsilon$ from *single.level.model* (hint: have a look at the function *model.matrix*)
    i.  create a plot that illustrates $x$, $Y$, $\hat{Y}$ and $\epsilon$ for subject 308)\
2.  Estimate $\beta$ for a quadratic model ($y = {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) *without* using *lm*; $\hat{\beta} = {({X}^{T} X)}^{-1} {X}^{T} Y$ (hint: add a third column to $X$ from step 1)
    i.  compare your acquired $\hat{\beta}$ with the output of a corresponding quadratic model created using *lm* (hint: use the function *I*, see details under help and the sub-section formula operators here: <https://www.datacamp.com/community/tutorials/r-formula-tutorial>)\
3.  Plot the data points using `plot(Reaction ~ Days, data=sleepstudy)`, then add the fitted lines from your linear model and from your quadratic model
    i.  Calculate the sum of squares for each of the two models, where $n$ is the number of observations $$SS = \sum_{i=1}^{n} (y_i - \hat{y_i})^2$$
    ii. Calculate the residual standard deviation, $\hat{\sigma} = \sqrt{\frac{SS}{n-k}}$ where $n$ is the number of observations and $k$ is the number of parameters fitted, for the two models (remember this is the estimated $sigma$ in $\epsilon$ \~ $N(0, \sigma^)$)
    iii. Based on the plot, the sums of squares and the $\hat{\sigma}$'s, which of the two models, do you prefer?
4.  Plot the design matrix, $X$ from the quadratic model, using *image.plot* from the *fields* package using the following code and explain what you see:
```{r, eval=FALSE}
library(fields)
image.plot(x=1:dim(X)[2], 1:dim(X)[1], t(X), xlab='Predictors',
           ylab='Observations', xaxt='n', main='Design matrix: X')
axis(1, 1:dim(X)[2], 1:dim(X)[2])
```

## Exercise 2

Do a multilevel linear regression, modelling reaction time as dependent of number of days of sleep deprivation, while accounting for the subject structure by modelling subject-level intercepts and slopes using the function *lmer*\
(Multilevel modelling: $Y = X\beta + Zb + \epsilon; \epsilon$ \~ $N(0, \sigma^2); b$ \~ $N(0, \Sigma); \Sigma$ is a covariance matrix )

```{r, eval=FALSE}
library(lme4)
multilevel.model <- lmer(formula=..., data=...)
```

1.  Extract $\hat{\beta}$, $Y$, $\hat{Y}$, $X$, $Z$, $\hat{b}$, $\hat{\Sigma}$, $\hat{\sigma}$ and $\epsilon$ from *multilevel.model* (hint: use the function *getME*)
    i.  create a plot that illustrates the fitted lines for each subject
2.  Plot the design matrices, $X$ and $Z$ using *image.plot* (note: you will have to apply *as.matrix* to $Z$ as *image.plot* doesn't handle sparse matrices)
    i.  Are there any differences between the current $X$ and the $X$ from Exercise 1.1 - why or why not?
    ii. Explain in your own words what $Z$ illustrates
3.  Finally, calculate the sum of squares and compare with the sum of squares for the *single.level.model*

## Exercise 3

Check whether your *Python* works from within *RStudio*:

```{r, eval=FALSE}
library(reticulate)
repl_python() ## this gives you a Python interpreter note that the console changes from > to >>>
```

You can now run code chunks like these in *Rstudio* by writing ´´´{python} instead of ´´´{r}

```{python}
# ```{python}
import numpy as np
a_range = np.arange(5)
print(a_range) ## note the zero-indexing
# ```
```

Access the California Housing dataset

```{python, eval=FALSE}
from sklearn.datasets import fetch_california_housing

california = fetch_california_housing()
x = california.data # independent variables; check california.feature_names
y = california.target # dependent variable: price actually sold for
```

1.  Run a single-level linear regression on the California Housing dataset from *scikit-learn* using algebra;
    i.  Build a model matrix $X$, with 2 predictors, an average (intercept) and a slope, predicting the prices houses were sold for based on the median income (MedInc) of the block (first column of *california_data*)
2.  Plot the fitted regression line and $Y$ and using `plt.plot`

Chunks for inspiration

```{python, eval=FALSE}
import numpy as np
X = np.zeros(shape=(n_obs, n_predictors) ## what are the n's?
X[:, 0] = ?# what should the entries in the first column be?
X[:, 1] = ?# what should the entries in the second column be?
beta_hat = ?# @ is matrix multiplication and np.linalg.inv is for inverting a matrix
```

```{python, eval=FALSE}
import matplotlib.pyplot as plt
plt.figure() # create figure
plt.plot(x[:, 0], y, 'o') # add points
x_pred = np.array([0, np.max(x[:, 0])])
y_pred = ?
plt.plot(x_pred, y_pred) ## add a line
plt.xlabel() ## add meaningful labels and a title
plt.ylabel()
plt.title()
plt.show() ## actually showing the figure
```
